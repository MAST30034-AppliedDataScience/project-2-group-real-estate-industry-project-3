{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:12:57.414049Z",
     "start_time": "2024-09-25T07:12:57.162134Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import functions as F  #filtering\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:13:03.369912Z",
     "start_time": "2024-09-25T07:12:57.414823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 17:12:58 WARN Utils: Your hostname, coldbrew.local resolves to a loopback address: 127.0.0.1; using 172.16.119.16 instead (on interface en0)\n",
      "24/09/25 17:12:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/25 17:12:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/25 17:12:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/25 17:12:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+--------------------+\n",
      "|                name|            sa2_name|postcode|            geometry|\n",
      "+--------------------+--------------------+--------+--------------------+\n",
      "|Lilydale-Warburto...|        Yarra Valley|    3139|POLYGON ((384396....|\n",
      "|Nangana Bushland ...|        Yarra Valley|    3139|POLYGON ((373830....|\n",
      "|Nillumbik G139 Bu...|Wattle Glen - Dia...|    3089|POLYGON ((339437....|\n",
      "|Lilydale-Warburto...|Lilydale - Coldst...|    3140|POLYGON ((355540....|\n",
      "|Plenty Gorge Park...|  Plenty - Yarrambat|    3088|POLYGON ((332381....|\n",
      "+--------------------+--------------------+--------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# starting a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName('Parkres Further Analysis')\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Read the dataset from a CSV file using PySpark\n",
    "parkres = spark.read.csv('../data/curated/parkres/parkres.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Drop the extra index column (_c0) if it exists\n",
    "parkres = parkres.drop('_c0')\n",
    "\n",
    "# Show the first few rows of the dataset to confirm\n",
    "parkres.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:13:04.806592Z",
     "start_time": "2024-09-25T07:13:03.371039Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 17:13:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "+---+--------------------+-----------------+--------------------+-------------+------------------+-----------+----+-----+-------+------+---------------+--------------------+---------+--------------------+--------+---------+--------+-----------------+--------+--------------------+--------+-----------------+--------+--------+--------+---------+--------+--------------------+\n|_c0|                 url|            price|             address|property_type|          latitude|  longitude|Beds|Baths|Parking|  bond|extracted_price|            geometry| sa2_code|            sa2_name|chg_flag|  chg_lbl|sa3_code|         sa3_name|sa4_code|            sa4_name|gcc_code|         gcc_name|ste_code|ste_name|aus_code| aus_name|areasqkm|            loci_uri|\n+---+--------------------+-----------------+--------------------+-------------+------------------+-----------+----+-----+-------+------+---------------+--------------------+---------+--------------------+--------+---------+--------+-----------------+--------+--------------------+--------+-----------------+--------+--------+--------+---------+--------+--------------------+\n|  0|https://www.domai...|        $1,400.00|10 Allara Court, ...|    Townhouse|-37.77427300000001|145.1811258| 4.0|  3.0|    2.0|9125.0|         1400.0|POINT (145.181125...|211021261|Donvale - Park Or...|       0|No change|   21102|Manningham - East|     211|Melbourne - Outer...|   2GMEL|Greater Melbourne|       2|Victoria|     AUS|Australia| 20.8028|http://linked.dat...|\n|  1|https://www.domai...|    $750 per week|7 Pine Ridge, Don...|        House|       -37.7912513|145.1756489| 4.0|  2.0|    0.0|3259.0|          750.0|POINT (145.175648...|211021261|Donvale - Park Or...|       0|No change|   21102|Manningham - East|     211|Melbourne - Outer...|   2GMEL|Greater Melbourne|       2|Victoria|     AUS|Australia| 20.8028|http://linked.dat...|\n|  2|https://www.domai...|   $1300 per week|20 Mulsanne Way, ...|        House|       -37.7972323|145.1812636| 5.0|  2.0|    2.0|5649.0|         1300.0|POINT (145.181263...|211021261|Donvale - Park Or...|       0|No change|   21102|Manningham - East|     211|Melbourne - Outer...|   2GMEL|Greater Melbourne|       2|Victoria|     AUS|Australia| 20.8028|http://linked.dat...|\n|  3|https://www.domai...|$825pw / $3585pcm|3 Monterey Cresce...|        House|        -37.792402|145.1743233| 3.0|  1.0|    5.0|3585.0|          825.0|POINT (145.174323...|211021261|Donvale - Park Or...|       0|No change|   21102|Manningham - East|     211|Melbourne - Outer...|   2GMEL|Greater Melbourne|       2|Victoria|     AUS|Australia| 20.8028|http://linked.dat...|\n|  4|https://www.domai...|          $680.00|3/49 Leslie Stree...|    Townhouse|       -37.7810117| 145.180705| 3.0|  2.0|    2.0|2955.0|          680.0|POINT (145.180705...|211021261|Donvale - Park Or...|       0|No change|   21102|Manningham - East|     211|Melbourne - Outer...|   2GMEL|Greater Melbourne|       2|Victoria|     AUS|Australia| 20.8028|http://linked.dat...|\n+---+--------------------+-----------------+--------------------+-------------+------------------+-----------+----+-----+-------+------+---------------+--------------------+---------+--------------------+--------+---------+--------+-----------------+--------+--------------------+--------+-----------------+--------+--------+--------+---------+--------+--------------------+",
      "text/html": "<table border='1'>\n<tr><th>_c0</th><th>url</th><th>price</th><th>address</th><th>property_type</th><th>latitude</th><th>longitude</th><th>Beds</th><th>Baths</th><th>Parking</th><th>bond</th><th>extracted_price</th><th>geometry</th><th>sa2_code</th><th>sa2_name</th><th>chg_flag</th><th>chg_lbl</th><th>sa3_code</th><th>sa3_name</th><th>sa4_code</th><th>sa4_name</th><th>gcc_code</th><th>gcc_name</th><th>ste_code</th><th>ste_name</th><th>aus_code</th><th>aus_name</th><th>areasqkm</th><th>loci_uri</th></tr>\n<tr><td>0</td><td>https://www.domai...</td><td>$1,400.00</td><td>10 Allara Court, ...</td><td>Townhouse</td><td>-37.77427300000001</td><td>145.1811258</td><td>4.0</td><td>3.0</td><td>2.0</td><td>9125.0</td><td>1400.0</td><td>POINT (145.181125...</td><td>211021261</td><td>Donvale - Park Or...</td><td>0</td><td>No change</td><td>21102</td><td>Manningham - East</td><td>211</td><td>Melbourne - Outer...</td><td>2GMEL</td><td>Greater Melbourne</td><td>2</td><td>Victoria</td><td>AUS</td><td>Australia</td><td>20.8028</td><td>http://linked.dat...</td></tr>\n<tr><td>1</td><td>https://www.domai...</td><td>$750 per week</td><td>7 Pine Ridge, Don...</td><td>House</td><td>-37.7912513</td><td>145.1756489</td><td>4.0</td><td>2.0</td><td>0.0</td><td>3259.0</td><td>750.0</td><td>POINT (145.175648...</td><td>211021261</td><td>Donvale - Park Or...</td><td>0</td><td>No change</td><td>21102</td><td>Manningham - East</td><td>211</td><td>Melbourne - Outer...</td><td>2GMEL</td><td>Greater Melbourne</td><td>2</td><td>Victoria</td><td>AUS</td><td>Australia</td><td>20.8028</td><td>http://linked.dat...</td></tr>\n<tr><td>2</td><td>https://www.domai...</td><td>$1300 per week</td><td>20 Mulsanne Way, ...</td><td>House</td><td>-37.7972323</td><td>145.1812636</td><td>5.0</td><td>2.0</td><td>2.0</td><td>5649.0</td><td>1300.0</td><td>POINT (145.181263...</td><td>211021261</td><td>Donvale - Park Or...</td><td>0</td><td>No change</td><td>21102</td><td>Manningham - East</td><td>211</td><td>Melbourne - Outer...</td><td>2GMEL</td><td>Greater Melbourne</td><td>2</td><td>Victoria</td><td>AUS</td><td>Australia</td><td>20.8028</td><td>http://linked.dat...</td></tr>\n<tr><td>3</td><td>https://www.domai...</td><td>$825pw / $3585pcm</td><td>3 Monterey Cresce...</td><td>House</td><td>-37.792402</td><td>145.1743233</td><td>3.0</td><td>1.0</td><td>5.0</td><td>3585.0</td><td>825.0</td><td>POINT (145.174323...</td><td>211021261</td><td>Donvale - Park Or...</td><td>0</td><td>No change</td><td>21102</td><td>Manningham - East</td><td>211</td><td>Melbourne - Outer...</td><td>2GMEL</td><td>Greater Melbourne</td><td>2</td><td>Victoria</td><td>AUS</td><td>Australia</td><td>20.8028</td><td>http://linked.dat...</td></tr>\n<tr><td>4</td><td>https://www.domai...</td><td>$680.00</td><td>3/49 Leslie Stree...</td><td>Townhouse</td><td>-37.7810117</td><td>145.180705</td><td>3.0</td><td>2.0</td><td>2.0</td><td>2955.0</td><td>680.0</td><td>POINT (145.180705...</td><td>211021261</td><td>Donvale - Park Or...</td><td>0</td><td>No change</td><td>21102</td><td>Manningham - East</td><td>211</td><td>Melbourne - Outer...</td><td>2GMEL</td><td>Greater Melbourne</td><td>2</td><td>Victoria</td><td>AUS</td><td>Australia</td><td>20.8028</td><td>http://linked.dat...</td></tr>\n</table>\n"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the domain parquet dataset\n",
    "domain = spark.read.parquet('../data/curated/domain_data')\n",
    "domain.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:13:05.484723Z",
     "start_time": "2024-09-25T07:13:04.816980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-------+-------------+--------+---------+----+-----+-------+----+---------------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|_c0|url|price|address|property_type|latitude|longitude|Beds|Baths|Parking|bond|extracted_price|geometry|sa2_code|sa2_name|chg_flag|chg_lbl|sa3_code|sa3_name|sa4_code|sa4_name|gcc_code|gcc_name|ste_code|ste_name|aus_code|aus_name|areasqkm|loci_uri|\n",
      "+---+---+-----+-------+-------------+--------+---------+----+-----+-------+----+---------------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|  0|  0|    0|      0|            0|       0|        0| 136|   67|      9|1199|              0|       0|       0|       0|       0|      0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|\n",
      "+---+---+-----+-------+-------------+--------+---------+----+-----+-------+----+---------------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check for null values in each column\n",
    "domain.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in domain.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:13:07.168042Z",
     "start_time": "2024-09-25T07:13:05.485525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 17:13:06 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 9)   \n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/_7/sr3p02zn35v2xg1fm8dsbz6c0000gn/T/ipykernel_1454/106316430.py\", line 12, in calculate_distance\n",
      "TypeError: must be real number, not str\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n",
      "\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n",
      "\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/09/25 17:13:06 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 9) (172.16.119.16 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/_7/sr3p02zn35v2xg1fm8dsbz6c0000gn/T/ipykernel_1454/106316430.py\", line 12, in calculate_distance\n",
      "TypeError: must be real number, not str\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n",
      "\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n",
      "\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/09/25 17:13:06 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/_7/sr3p02zn35v2xg1fm8dsbz6c0000gn/T/ipykernel_1454/106316430.py\", line 12, in calculate_distance\nTypeError: must be real number, not str\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 51\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Calculate distances between properties and park centroids\u001B[39;00m\n\u001B[1;32m     48\u001B[0m result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistance\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[1;32m     49\u001B[0m     distance_udf(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprop_lat\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprop_lon\u001B[39m\u001B[38;5;124m\"\u001B[39m), \n\u001B[1;32m     50\u001B[0m                  F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpark_centroid_lat\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpark_centroid_lon\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[0;32m---> 51\u001B[0m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morderBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdistance\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/project-2-group-real-estate-industry-project-3/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    888\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001B[39;00m\n\u001B[1;32m    889\u001B[0m \n\u001B[1;32m    890\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    945\u001B[0m \u001B[38;5;124;03m    name | Bob\u001B[39;00m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 947\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/project-2-group-real-estate-industry-project-3/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:965\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    959\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    960\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    961\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    962\u001B[0m     )\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    967\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/project-2-group-real-estate-industry-project-3/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/PycharmProjects/project-2-group-real-estate-industry-project-3/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/_7/sr3p02zn35v2xg1fm8dsbz6c0000gn/T/ipykernel_1454/106316430.py\", line 12, in calculate_distance\nTypeError: must be real number, not str\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from shapely import wkt\n",
    "from pyspark.sql.functions import udf\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Function to calculate distance between two points (Haversine formula)\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in km\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# UDF to convert WKT geometry to centroid coordinates\n",
    "def get_centroid(geometry):\n",
    "    try:\n",
    "        shape = wkt.loads(geometry)\n",
    "        centroid = shape.centroid\n",
    "        return f\"{centroid.y},{centroid.x}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Register UDFs\n",
    "distance_udf = udf(calculate_distance, DoubleType())\n",
    "centroid_udf = udf(get_centroid, StringType())\n",
    "\n",
    "# Prepare park data\n",
    "parkres = parkres.withColumnRenamed(\"geometry\", \"park_geometry\")\n",
    "parkres = parkres.withColumn(\"park_centroid\", centroid_udf(F.col(\"park_geometry\")))\n",
    "parkres = parkres.withColumn(\"park_centroid_lat\", F.split(F.col(\"park_centroid\"), \",\")[0].cast(DoubleType()))\n",
    "parkres = parkres.withColumn(\"park_centroid_lon\", F.split(F.col(\"park_centroid\"), \",\")[1].cast(DoubleType()))\n",
    "\n",
    "# Prepare domain data\n",
    "domain = domain.withColumnRenamed(\"latitude\", \"prop_lat\")\n",
    "domain = domain.withColumnRenamed(\"longitude\", \"prop_lon\")\n",
    "domain = domain.withColumnRenamed(\"sa2_name\", \"property_sa2_name\")\n",
    "\n",
    "# Cross join to calculate the distance between each property and every park\n",
    "result = domain.crossJoin(F.broadcast(parkres))\n",
    "\n",
    "# Calculate distances between properties and park centroids\n",
    "result = result.withColumn(\"distance\", \n",
    "    distance_udf(F.col(\"prop_lat\"), F.col(\"prop_lon\"), \n",
    "                 F.col(\"park_centroid_lat\"), F.col(\"park_centroid_lon\")))\n",
    "result.orderBy(\"distance\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Find the nearest park for each property\n",
    "window_spec = Window.partitionBy(\"__index_level_0__\").orderBy(\"distance\")\n",
    "nearest_park = result.withColumn(\"row\", F.row_number().over(window_spec)) \\\n",
    "    .filter(F.col(\"row\") == 1) \\\n",
    "    .select(\n",
    "        \"__index_level_0__\",\n",
    "        F.col(\"distance\").alias(\"nearest_park_distance\"),\n",
    "        F.col(\"name\").alias(\"nearest_park_name\"),\n",
    "        F.col(\"sa2_name\").alias(\"park_sa2_name\")\n",
    "    )\n",
    "\n",
    "# Join back to the original domain data\n",
    "final_result = domain.join(nearest_park, on=\"__index_level_0__\")\n",
    "\n",
    "# Show the result for validation\n",
    "final_result.select(\n",
    "    \"address\", \n",
    "    \"extracted_price\", \n",
    "    \"nearest_park_distance\", \n",
    "    \"nearest_park_name\", \n",
    "    \"property_sa2_name\", \n",
    "    \"park_sa2_name\"\n",
    ") \\\n",
    "    .orderBy(\"nearest_park_distance\") \\\n",
    "    .show(10, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Helper function to parse geometry\n",
    "def parse_geometry(geom):\n",
    "    coords = F.split(F.regexp_extract(geom, r\"POINT\\((.*?)\\)\", 1), \" \")\n",
    "    return F.struct(\n",
    "        coords[0].cast(DoubleType()).alias(\"longitude\"),\n",
    "        coords[1].cast(DoubleType()).alias(\"latitude\")\n",
    "    )\n",
    "\n",
    "# Haversine distance function with null handling\n",
    "@F.udf(returnType=DoubleType())\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:\n",
    "        return None\n",
    "    \n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Load data\n",
    "parkres = spark.read.csv('../data/curated/parkres/parkres.csv', header=True, inferSchema=True)\n",
    "domain = spark.read.parquet('../data/curated/domain_data')\n",
    "\n",
    "# Display schema and sample data\n",
    "print(\"parkres schema:\")\n",
    "parkres.printSchema()\n",
    "print(\"\\nSample parkres data:\")\n",
    "parkres.show(5, truncate=False)\n",
    "\n",
    "print(\"\\ndomain schema:\")\n",
    "domain.printSchema()\n",
    "print(\"\\nSample domain data:\")\n",
    "domain.show(5, truncate=False)\n",
    "\n",
    "# Parse geometry for parkres\n",
    "parkres = parkres.withColumn(\"park_coords\", parse_geometry(F.col(\"geometry\")))\n",
    "\n",
    "# Prepare domain data\n",
    "domain = domain.select(\n",
    "    \"url\", \"extracted_price\", \n",
    "    F.col(\"latitude\").cast(DoubleType()),\n",
    "    F.col(\"longitude\").cast(DoubleType())\n",
    ").withColumn(\"property_coords\", F.struct(\"latitude\", \"longitude\"))\n",
    "\n",
    "# Calculate distances using cross join\n",
    "crossed = domain.crossJoin(F.broadcast(parkres))\n",
    "with_distances = crossed.withColumn(\"distance_to_park\",\n",
    "    haversine_distance(\n",
    "        F.col(\"property_coords.latitude\"), F.col(\"property_coords.longitude\"),\n",
    "        F.col(\"park_coords.latitude\"), F.col(\"park_coords.longitude\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Find nearest park for each property\n",
    "nearest_park = with_distances.groupBy(\"url\").agg(\n",
    "    F.min(\"distance_to_park\").alias(\"nearest_park_distance\")\n",
    ")\n",
    "\n",
    "# Join back to get full property data with nearest park distance\n",
    "result = domain.join(nearest_park, on=\"url\")\n",
    "\n",
    "# Analyze by distance ranges\n",
    "analysis = result.groupBy(F.round(F.col(\"nearest_park_distance\")).alias(\"distance_km\")) \\\n",
    "    .agg(\n",
    "        F.avg(\"extracted_price\").alias(\"avg_price\"),\n",
    "        F.count(\"url\").alias(\"property_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"distance_km\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nFinal analysis:\")\n",
    "analysis.show()\n",
    "\n",
    "# Optional: Count null distances\n",
    "null_distances = result.filter(F.col(\"nearest_park_distance\").isNull()).count()\n",
    "print(f\"\\nNumber of properties with null distances: {null_distances}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Inspect the domain dataset\n",
    "print(domain.show(5))  # Show a few rows to ensure data exists in 'latitude' and 'longitude'\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "domain_pd = domain.select('url', 'latitude', 'longitude').toPandas()\n",
    "\n",
    "# Step 2: Check for null values in domain's latitude and longitude\n",
    "print(\"Checking for null values in domain dataset:\")\n",
    "print(domain_pd.isnull().sum())  # This will show if there are any null latitudes or longitudes\n",
    "\n",
    "# Step 3: Check if lat/lon columns are valid in the domain\n",
    "domain_pd = domain_pd.dropna(subset=['latitude', 'longitude'])\n",
    "print(f\"Number of valid rows in domain dataset after dropping nulls: {len(domain_pd)}\")\n",
    "\n",
    "# Step 4: Inspect the parkres dataset\n",
    "print(parkres.show(5))  # Show a few rows to ensure data exists in 'geometry'\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "parkres_pd = parkres.select('sa2_name', 'geometry').toPandas()\n",
    "\n",
    "# Step 5: Check for null values in parkres dataset\n",
    "print(\"Checking for null values in parkres dataset:\")\n",
    "print(parkres_pd.isnull().sum())  # This will show if there are any null geometries\n",
    "\n",
    "# Step 6: Check if geometry column is valid in parkres\n",
    "parkres_pd = parkres_pd.dropna(subset=['geometry'])\n",
    "print(f\"Number of valid rows in parkres dataset after dropping null geometries: {len(parkres_pd)}\")\n",
    "\n",
    "# Step 7: Try converting both datasets to GeoDataFrames\n",
    "if not domain_pd.empty and not parkres_pd.empty:\n",
    "    # Create GeoDataFrame for domain\n",
    "    domain_gdf = gpd.GeoDataFrame(domain_pd, geometry=gpd.points_from_xy(domain_pd.longitude, domain_pd.latitude), crs=\"EPSG:4326\")\n",
    "\n",
    "    # Create GeoDataFrame for parkres\n",
    "    parkres_gdf = gpd.GeoDataFrame(parkres_pd, geometry=parkres_pd['geometry'].apply(wkt.loads), crs=\"EPSG:4326\")\n",
    "\n",
    "    print(\"Successfully created GeoDataFrames for both datasets.\")\n",
    "else:\n",
    "    print(\"One of the datasets is still empty after handling missing values.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from shapely import wkb, wkt\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "# Define UDF for geospatial operations\n",
    "@F.udf(returnType=DoubleType())\n",
    "def calculate_distance(lat, lon, park_geom_wkt):\n",
    "    point = wkt.loads(f\"POINT({lon} {lat})\")\n",
    "    park = wkt.loads(park_geom_wkt)\n",
    "    nearest_point = nearest_points(point, park.exterior)[1]\n",
    "    return point.distance(nearest_point) * 111000  # Approximate conversion to meters\n",
    "\n",
    "# Convert binary WKB to WKT\n",
    "def wkb_to_wkt(wkb_value):\n",
    "    if wkb_value is None:\n",
    "        return None\n",
    "    return wkb.loads(wkb_value).wkt\n",
    "\n",
    "wkb_to_wkt_udf = F.udf(wkb_to_wkt, returnType=F.StringType())\n",
    "\n",
    "# Prepare domain DataFrame\n",
    "domain_prepared = domain.withColumn(\"domain_geometry_wkt\", wkb_to_wkt_udf(F.col(\"geometry\")))\n",
    "\n",
    "# Rename parkres geometry column to avoid conflict\n",
    "parkres_prepared = parkres.withColumnRenamed(\"geometry\", \"park_geometry\")\n",
    "\n",
    "# Cross join domain with parkres\n",
    "joined = domain_prepared.crossJoin(parkres_prepared)\n",
    "\n",
    "# Calculate distances\n",
    "joined = joined.withColumn(\"distance\", \n",
    "                           calculate_distance(F.col(\"latitude\"), \n",
    "                                              F.col(\"longitude\"), \n",
    "                                              F.col(\"park_geometry\")))\n",
    "\n",
    "# Find the minimum distance for each domain entry\n",
    "result = joined.groupBy(\"url\", \"address\", \"__index_level_0__\") \\\n",
    "    .agg(F.min(\"distance\").alias(\"min_distance_to_park_border\"), \n",
    "         F.first(\"name\").alias(\"nearest_park_name\"))\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False)\n",
    "\n",
    "# If you want to save the results\n",
    "# result.write.parquet(\"path/to/save/results.parquet\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from shapely import wkb, wkt\n",
    "from shapely.ops import nearest_points\n",
    "import math\n",
    "\n",
    "# Define the Haversine formula to calculate distance in meters between two lat/lon points\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c  # Distance in meters\n",
    "\n",
    "# Define UDF for geospatial operations using Haversine formula\n",
    "@F.udf(returnType=DoubleType())\n",
    "def calculate_distance(lat, lon, park_geom_wkt):\n",
    "    point = wkt.loads(f\"POINT({lon} {lat})\")\n",
    "    park = wkt.loads(park_geom_wkt)\n",
    "    \n",
    "    # Find the nearest point on the park boundary\n",
    "    nearest_point = nearest_points(point, park.exterior)[1]\n",
    "    \n",
    "    # Calculate the Haversine distance between the property point and nearest park point\n",
    "    return haversine_distance(lat, lon, nearest_point.y, nearest_point.x)\n",
    "\n",
    "# Convert binary WKB to WKT\n",
    "def wkb_to_wkt(wkb_value):\n",
    "    if wkb_value is None:\n",
    "        return None\n",
    "    return wkb.loads(wkb_value).wkt\n",
    "\n",
    "wkb_to_wkt_udf = F.udf(wkb_to_wkt, returnType=F.StringType())\n",
    "\n",
    "# Prepare domain DataFrame\n",
    "domain_prepared = domain.withColumn(\"domain_geometry_wkt\", wkb_to_wkt_udf(F.col(\"geometry\")))\n",
    "\n",
    "# Rename parkres geometry column to avoid conflict\n",
    "parkres_prepared = parkres.withColumnRenamed(\"geometry\", \"park_geometry\")\n",
    "\n",
    "# Cross join domain with parkres\n",
    "joined = domain_prepared.crossJoin(parkres_prepared)\n",
    "\n",
    "# Calculate distances using the improved method (Haversine formula)\n",
    "joined = joined.withColumn(\"distance\", \n",
    "                           calculate_distance(F.col(\"latitude\"), \n",
    "                                              F.col(\"longitude\"), \n",
    "                                              F.col(\"park_geometry\")))\n",
    "\n",
    "# Find the minimum distance for each domain entry\n",
    "result = joined.groupBy(\"url\", \"address\", \"__index_level_0__\") \\\n",
    "    .agg(F.min(\"distance\").alias(\"min_distance_to_park_border\"), \n",
    "         F.first(\"name\").alias(\"nearest_park_name\"))\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Optional: Save the results if needed\n",
    "# result.write.parquet(\"path/to/save/results.parquet\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from shapely import wkb, wkt\n",
    "import math\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of Earth in kilometers\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    \n",
    "    return R * c * 1000  # Distance in meters\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def calculate_distance(lat, lon, park_geom_wkt):\n",
    "    if lat is None or lon is None or park_geom_wkt is None:\n",
    "        return None\n",
    "    \n",
    "    point = wkt.loads(f\"POINT({lon} {lat})\")\n",
    "    park = wkt.loads(park_geom_wkt)\n",
    "    \n",
    "    # Find the nearest point on the park boundary\n",
    "    nearest_point = park.exterior.interpolate(park.exterior.project(point))\n",
    "    \n",
    "    return haversine_distance(lat, lon, nearest_point.y, nearest_point.x)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def wkb_to_wkt(wkb_value):\n",
    "    if wkb_value is None:\n",
    "        return None\n",
    "    return wkb.loads(wkb_value).wkt\n",
    "\n",
    "# Prepare domain DataFrame\n",
    "domain_prepared = domain.withColumn(\"domain_geometry_wkt\", wkb_to_wkt(domain.geometry))\n",
    "\n",
    "# Rename parkres geometry column to avoid conflict\n",
    "parkres_prepared = parkres.withColumnRenamed(\"geometry\", \"park_geometry\")\n",
    "\n",
    "# Join domain with parkres\n",
    "joined = domain_prepared.crossJoin(parkres_prepared)\n",
    "\n",
    "# Calculate distances\n",
    "joined = joined.withColumn(\"distance\",\n",
    "                           calculate_distance(joined.latitude,\n",
    "                                              joined.longitude,\n",
    "                                              joined.park_geometry))\n",
    "\n",
    "# Find the minimum distance for each domain entry\n",
    "result = joined.groupBy(\"url\", \"address\", \"__index_level_0__\") \\\n",
    "    .agg({\"distance\": \"min\", \"name\": \"first\"}) \\\n",
    "    .withColumnRenamed(\"min(distance)\", \"min_distance_to_park_border\") \\\n",
    "    .withColumnRenamed(\"first(name)\", \"nearest_park_name\")\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Optional: Save the results if needed\n",
    "# result.write.parquet(\"path/to/save/results.parquet\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
    "import shapely\n",
    "from shapely import wkb, wkt\n",
    "from pyproj import Geod\n",
    "import sys\n",
    "\n",
    "# Use WGS84 ellipsoid for distance calculations\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Define a struct to return both distance and error message\n",
    "result_schema = StructType([\n",
    "    StructField(\"distance\", DoubleType(), True),\n",
    "    StructField(\"error\", StringType(), True)\n",
    "])\n",
    "\n",
    "@udf(returnType=result_schema)\n",
    "def calculate_distance(lat, lon, park_geom_wkt):\n",
    "    try:\n",
    "        if lat is None or lon is None or park_geom_wkt is None:\n",
    "            return (None, \"One or more input values are None\")\n",
    "        \n",
    "        lat, lon = float(lat), float(lon)\n",
    "        point = wkt.loads(f\"POINT({lon} {lat})\")\n",
    "        park = wkt.loads(park_geom_wkt)\n",
    "        \n",
    "        # Find the nearest point on the park boundary\n",
    "        nearest_point = park.exterior.interpolate(park.exterior.project(point))\n",
    "        \n",
    "        # Calculate the geodesic distance\n",
    "        _, _, distance = geod.inv(lon, lat, nearest_point.x, nearest_point.y)\n",
    "        \n",
    "        return (float(distance), None)\n",
    "    except Exception as e:\n",
    "        return (None, str(e))\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def wkb_to_wkt(wkb_value):\n",
    "    try:\n",
    "        if wkb_value is None:\n",
    "            return None\n",
    "        return wkb.loads(wkb_value).wkt\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Prepare domain DataFrame\n",
    "domain_prepared = domain.withColumn(\"domain_geometry_wkt\", wkb_to_wkt(col(\"geometry\")))\n",
    "\n",
    "# Rename parkres geometry column to avoid conflict\n",
    "parkres_prepared = parkres.withColumnRenamed(\"geometry\", \"park_geometry\")\n",
    "\n",
    "# Join domain with parkres\n",
    "joined = domain_prepared.crossJoin(parkres_prepared)\n",
    "\n",
    "# Calculate distances\n",
    "distance_calc = calculate_distance(col(\"latitude\"), col(\"longitude\"), col(\"park_geometry\"))\n",
    "joined = joined.withColumn(\"distance_result\", distance_calc)\n",
    "joined = joined.withColumn(\"distance\", col(\"distance_result.distance\"))\n",
    "joined = joined.withColumn(\"distance_error\", col(\"distance_result.error\"))\n",
    "\n",
    "# Find the minimum distance for each domain entry\n",
    "result = joined.groupBy(\"url\", \"address\", \"__index_level_0__\") \\\n",
    "    .agg({\"distance\": \"min\", \"name\": \"first\", \"distance_error\": \"first\"}) \\\n",
    "    .withColumnRenamed(\"min(distance)\", \"min_distance_to_park_border\") \\\n",
    "    .withColumnRenamed(\"first(name)\", \"nearest_park_name\") \\\n",
    "    .withColumnRenamed(\"first(distance_error)\", \"error_message\")\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Optional: Save the results if needed\n",
    "# result.write.parquet(\"path/to/save/results.parquet\")\n",
    "\n",
    "# Print some debug information\n",
    "print(\"Sample of domain_geometry_wkt:\")\n",
    "domain_prepared.select(\"domain_geometry_wkt\").show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSample of park_geometry:\")\n",
    "parkres_prepared.select(\"park_geometry\").show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSample of latitude and longitude:\")\n",
    "domain_prepared.select(\"latitude\", \"longitude\").show(5, truncate=False)\n",
    "\n",
    "print(\"\\nUnique error messages:\")\n",
    "result.select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sqrt, pow, min as spark_min, first\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import radians\n",
    "\n",
    "# Approximate radius of earth in km\n",
    "R = 6371.0\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = pow(sin(dlat/2), 2) + cos(lat1) * cos(lat2) * pow(sin(dlon/2), 2)\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Register the UDF\n",
    "haversine_udf = F.udf(haversine_distance, DoubleType())\n",
    "\n",
    "# Assuming 'domain' is your DataFrame with property data\n",
    "# and 'parkres' is your DataFrame with park data\n",
    "\n",
    "# Calculate the distance for each property-park pair\n",
    "result = domain.crossJoin(parkres) \\\n",
    "    .withColumn(\"distance\", \n",
    "                haversine_udf(\n",
    "                    col(\"latitude\"), col(\"longitude\"), \n",
    "                    col(\"park_latitude\"), col(\"park_longitude\")\n",
    "                )) \\\n",
    "    .groupBy(\"url\", \"address\", \"__index_level_0__\") \\\n",
    "    .agg(\n",
    "        spark_min(\"distance\").alias(\"min_distance_to_park_border\"),\n",
    "        first(\"park_name\").alias(\"nearest_park_name\")\n",
    "    )\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Optional: Save the results if needed\n",
    "# result.write.parquet(\"path/to/save/results.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
