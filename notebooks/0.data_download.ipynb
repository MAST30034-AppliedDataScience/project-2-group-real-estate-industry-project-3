{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_relative_dir = '../data/'\n",
    "# Ensure the base directory exists\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "\n",
    "# List of folder types and corresponding subfolders\n",
    "folder_structure = {\n",
    "    'landing': ['criminal_incidents', 'property_data'],\n",
    "    'raw': ['criminal_incidents', 'property_data'],\n",
    "    'processed': ['criminal_incidents', 'property_data']\n",
    "}\n",
    "\n",
    "# Create folders based on the structure\n",
    "for folder_type, subfolders in folder_structure.items():\n",
    "    base_path = os.path.join(output_relative_dir, folder_type)\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(base_path, subfolder)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet names (tables and other data): ['Contents', 'Footnotes', 'Table 01', 'Table 02', 'Table 03', 'Table 04', 'Table 05']\n",
      "Table 'Table 01' saved successfully as ../data/landing/criminal_incidents/criminal_table_01.parquet\n",
      "Table 'Table 02' saved successfully as ../data/landing/criminal_incidents/criminal_table_02.parquet\n",
      "Table 'Table 03' saved successfully as ../data/landing/criminal_incidents/criminal_table_03.parquet\n",
      "Table 'Table 04' saved successfully as ../data/landing/criminal_incidents/criminal_table_04.parquet\n",
      "Table 'Table 05' saved successfully as ../data/landing/criminal_incidents/criminal_table_05.parquet\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory for storing the Parquet files\n",
    "base_dir = '../data/landing/criminal_incidents/'\n",
    "\n",
    "# Ensure the base directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# File URL for downloading the Excel file\n",
    "url = 'https://files.crimestatistics.vic.gov.au/2024-06/Data_Tables_LGA_Criminal_Incidents_Year_Ending_March_2024.xlsx'\n",
    "\n",
    "# Download the Excel file content directly into memory\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Load the Excel content into a pandas ExcelFile object directly from memory\n",
    "    xls = pd.ExcelFile(response.content)\n",
    "\n",
    "    # Get all sheet names (table names)\n",
    "    table_names = xls.sheet_names\n",
    "    print(\"Sheet names (tables and other data):\", table_names)\n",
    "\n",
    "    # Initialize a counter for table naming\n",
    "    table_counter = 1\n",
    "\n",
    "    # Iterate over each sheet and save only tables as Parquet files\n",
    "    for sheet in table_names:\n",
    "        # Assuming table sheets contain the word 'Table'\n",
    "        if 'Table' in sheet or sheet.lower().startswith('table'):\n",
    "            # Read the sheet into a DataFrame\n",
    "            df = pd.read_excel(xls, sheet_name=sheet)\n",
    "\n",
    "            # Save the table as a Parquet file with the naming convention 'criminal_table_X.parquet'\n",
    "            parquet_file_name = f'criminal_table_{table_counter:02}.parquet'\n",
    "            parquet_output_path = os.path.join(base_dir, parquet_file_name)\n",
    "            df.to_parquet(parquet_output_path, index=False)\n",
    "            print(f\"Table '{sheet}' saved successfully as {parquet_output_path}\")\n",
    "\n",
    "            # Increment table counter for each saved table\n",
    "            table_counter += 1\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet names (tables and other data): ['Contents', 'Footnotes', 'Table 01', 'Table 02', 'Table 03', 'Table 04', 'Table 05', 'Table 06']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Iterate over each sheet and save them accordingly\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sheet \u001b[38;5;129;01min\u001b[39;00m sheet_names:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Read the sheet into a DataFrame\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(xls, sheet_name\u001b[38;5;241m=\u001b[39msheet)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Only process and save sheets that are tables (skip \"Contents\" and \"Footnotes\")\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sheet \u001b[38;5;129;01mor\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# Handle inconsistent data types by converting all columns to strings\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:490\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m     data \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    491\u001b[0m         sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[0;32m    492\u001b[0m         header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m    493\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    494\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    495\u001b[0m         usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m    496\u001b[0m         squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[0;32m    497\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    498\u001b[0m         converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m    499\u001b[0m         true_values\u001b[38;5;241m=\u001b[39mtrue_values,\n\u001b[0;32m    500\u001b[0m         false_values\u001b[38;5;241m=\u001b[39mfalse_values,\n\u001b[0;32m    501\u001b[0m         skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m    502\u001b[0m         nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m    503\u001b[0m         na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m    504\u001b[0m         keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[0;32m    505\u001b[0m         na_filter\u001b[38;5;241m=\u001b[39mna_filter,\n\u001b[0;32m    506\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    507\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    508\u001b[0m         date_parser\u001b[38;5;241m=\u001b[39mdate_parser,\n\u001b[0;32m    509\u001b[0m         thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m    510\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m    511\u001b[0m         comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[0;32m    512\u001b[0m         skipfooter\u001b[38;5;241m=\u001b[39mskipfooter,\n\u001b[0;32m    513\u001b[0m         convert_float\u001b[38;5;241m=\u001b[39mconvert_float,\n\u001b[0;32m    514\u001b[0m         mangle_dupe_cols\u001b[38;5;241m=\u001b[39mmangle_dupe_cols,\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1734\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1702\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;124;03m        DataFrame from the passed in Excel file.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m   1735\u001b[0m         sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[0;32m   1736\u001b[0m         header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1737\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m   1738\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1739\u001b[0m         usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1740\u001b[0m         squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[0;32m   1741\u001b[0m         converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1742\u001b[0m         true_values\u001b[38;5;241m=\u001b[39mtrue_values,\n\u001b[0;32m   1743\u001b[0m         false_values\u001b[38;5;241m=\u001b[39mfalse_values,\n\u001b[0;32m   1744\u001b[0m         skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1745\u001b[0m         nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m   1746\u001b[0m         na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1747\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1748\u001b[0m         date_parser\u001b[38;5;241m=\u001b[39mdate_parser,\n\u001b[0;32m   1749\u001b[0m         thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1750\u001b[0m         comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[0;32m   1751\u001b[0m         skipfooter\u001b[38;5;241m=\u001b[39mskipfooter,\n\u001b[0;32m   1752\u001b[0m         convert_float\u001b[38;5;241m=\u001b[39mconvert_float,\n\u001b[0;32m   1753\u001b[0m         mangle_dupe_cols\u001b[38;5;241m=\u001b[39mmangle_dupe_cols,\n\u001b[0;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1755\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:765\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[0;32m    764\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[1;32m--> 765\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_data(sheet, convert_float, file_rows_needed)\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:615\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[1;34m(self, sheet, convert_float, file_rows_needed)\u001b[0m\n\u001b[0;32m    613\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[0;32m    616\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell, convert_float) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:79\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[1;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[0;32m     75\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_source()\n\u001b[0;32m     76\u001b[0m parser \u001b[38;5;241m=\u001b[39m WorkSheetParser(src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_strings,\n\u001b[0;32m     77\u001b[0m                          data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mdata_only, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[0;32m     78\u001b[0m                          date_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_date_formats)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse():\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m max_row:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:144\u001b[0m, in \u001b[0;36mparse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:1251\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m data \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\zipfile.py:953\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[1;32m--> 953\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read1(n)\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\zipfile.py:1029\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1028\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1029\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(data, n)\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the output directory inside WSL\n",
    "output_relative_dir = '../data/landing/property_data/'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "\n",
    "# File URL for downloading the Excel file\n",
    "url = 'https://files.crimestatistics.vic.gov.au/2024-06/Data_Tables_Property_Items_Visualisation_Year_Ending_March_2024.xlsx'\n",
    "\n",
    "# Download the Excel file content directly into memory\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Load the Excel content into a pandas ExcelFile object directly from memory\n",
    "    xls = pd.ExcelFile(response.content)\n",
    "\n",
    "    # Get all sheet names (table names)\n",
    "    sheet_names = xls.sheet_names\n",
    "    print(\"Sheet names (tables and other data):\", sheet_names)\n",
    "\n",
    "    # Initialize a counter for table naming\n",
    "    table_counter = 1\n",
    "\n",
    "    # Iterate over each sheet and save them accordingly\n",
    "    for sheet in sheet_names:\n",
    "        # Read the sheet into a DataFrame\n",
    "        df = pd.read_excel(xls, sheet_name=sheet)\n",
    "\n",
    "        # Only process and save sheets that are tables (skip \"Contents\" and \"Footnotes\")\n",
    "        if 'Table' in sheet or sheet.lower().startswith('table'):\n",
    "            # Handle inconsistent data types by converting all columns to strings\n",
    "            df = df.astype(str)\n",
    "\n",
    "            # Save the table as a Parquet file with the naming convention 'property_table_X.parquet'\n",
    "            parquet_file_name = f'property_table_{table_counter:02}.parquet'\n",
    "            parquet_output_path = os.path.join(output_relative_dir, parquet_file_name)\n",
    "            df.to_parquet(parquet_output_path, index=False)\n",
    "            print(f\"Table '{sheet}' saved successfully as {parquet_output_path}\")\n",
    "\n",
    "            # Increment table counter for each saved table\n",
    "            table_counter += 1\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SA2_district_boundaries.dbf',\n",
       " 'SA2_district_boundaries.shx',\n",
       " 'SA2_2021_AUST_SHP_GDA2020.zip',\n",
       " 'SA2_district_boundaries.shp',\n",
       " 'SA2_district_boundaries.prj',\n",
       " 'SA2_district_boundaries.xml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL for the ZIP file\n",
    "url = 'https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_SHP_GDA94.zip'\n",
    "\n",
    "# Define the output directory and file name\n",
    "output_dir = '../data/landing/boundaries/'\n",
    "zip_file_path = os.path.join(output_dir, 'SA2_2021_AUST_SHP_GDA2020.zip')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Download the ZIP file and save it locally\n",
    "response = requests.get(url)\n",
    "with open(zip_file_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Extract and rename all files in the ZIP archive\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all files to the output directory\n",
    "    zip_ref.extractall(output_dir)\n",
    "    # List all files in the ZIP archive\n",
    "    file_list = zip_ref.namelist()\n",
    "\n",
    "    # Rename each file in the output directory to start with 'SA2_district_boundaries'\n",
    "    for original_file in file_list:\n",
    "        original_path = os.path.join(output_dir, original_file)\n",
    "        # Get the file extension\n",
    "        file_extension = os.path.splitext(original_file)[1]\n",
    "        # Define the new file name\n",
    "        new_file_name = f\"SA2_district_boundaries{file_extension}\"\n",
    "        new_file_path = os.path.join(output_dir, new_file_name)\n",
    "        # Rename the file if it exists\n",
    "        if os.path.exists(original_path):\n",
    "            os.rename(original_path, new_file_path)\n",
    "\n",
    "# Return the list of renamed files\n",
    "renamed_files = os.listdir(output_dir)\n",
    "renamed_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as Parquet at: ../data/landing/suburb_match/suburb_match.parquet\n",
      "['suburb_match.csv', 'suburb_match.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for the CSV file\n",
    "url = 'https://www.matthewproctor.com/Content/postcodes/australian_postcodes.csv'\n",
    "\n",
    "# Define the output directory and file name (using current directory)\n",
    "output_dir = '../data/landing/suburb_match/'\n",
    "csv_file_path = os.path.join(output_dir, 'suburb_match.csv')\n",
    "parquet_file_path = os.path.join(output_dir, 'suburb_match.parquet')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Download the CSV file and save it locally\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(csv_file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Save the DataFrame as a Parquet file\n",
    "    df.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "    # Return the path of the saved Parquet file\n",
    "    print(f\"File saved as Parquet at: {parquet_file_path}\")\n",
    "else:\n",
    "    print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n",
    "# Return the list of files in the output directory\n",
    "downloaded_files = os.listdir(output_dir)\n",
    "print(downloaded_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parkres.zip',\n",
       " 'parkres.prj',\n",
       " 'parkres.cpg',\n",
       " 'parkres.dbf',\n",
       " 'parkres.txt',\n",
       " 'll_gda94',\n",
       " 'parkres.shx',\n",
       " 'parkres.shp',\n",
       " 'parkres.html']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL for the ZIP file\n",
    "url = 'https://s3.ap-southeast-2.amazonaws.com/cl-isd-prd-datashare-s3-delivery/Order_Y9LSRC.zip'\n",
    "\n",
    "# Define the output directory and file name\n",
    "output_dir = '../data/landing/parkres/'\n",
    "zip_file_path = os.path.join(output_dir, 'parkres.zip')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Download the ZIP file and save it locally\n",
    "response = requests.get(url)\n",
    "with open(zip_file_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Extract and rename all files in the ZIP archive\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all files to the output directory\n",
    "    zip_ref.extractall(output_dir)\n",
    "    # List all files in the ZIP archive\n",
    "    file_list = zip_ref.namelist()\n",
    "\n",
    "    # Rename each file in the output directory to start with 'parkres'\n",
    "    for original_file in file_list:\n",
    "        original_path = os.path.join(output_dir, original_file)\n",
    "        # Get the file extension\n",
    "        file_extension = os.path.splitext(original_file)[1]\n",
    "        # Define the new file name\n",
    "        new_file_name = f\"parkres{file_extension}\"\n",
    "        new_file_path = os.path.join(output_dir, new_file_name)\n",
    "        # Rename the file if it exists\n",
    "        if os.path.exists(original_path):\n",
    "            os.rename(original_path, new_file_path)\n",
    "\n",
    "# Return the list of renamed files\n",
    "renamed_files = os.listdir(output_dir)\n",
    "renamed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file downloaded and saved at: ../data/landing/postcode/postcode_ref.xls\n",
      "File converted to CSV and saved at: ../data/landing/postcode/postcode_ref.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the file to download\n",
    "url = \"https://www.health.vic.gov.au/sites/default/files/2024-07/postcode-locality-reference.xls\"\n",
    "\n",
    "# Define the output directory and file name\n",
    "output_dir = '../data/landing/postcode/'  # Change this to your desired directory\n",
    "xls_file_name = 'postcode_ref.xls'\n",
    "csv_file_name = 'postcode_ref.csv'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Full path to save the Excel and CSV files\n",
    "xls_file_path = os.path.join(output_dir, xls_file_name)\n",
    "csv_file_path = os.path.join(output_dir, csv_file_name)\n",
    "\n",
    "# Download the Excel file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the download was successful (HTTP 200 OK)\n",
    "if response.status_code == 200:\n",
    "    # Write the Excel file content to the local system\n",
    "    with open(xls_file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Excel file downloaded and saved at: {xls_file_path}\")\n",
    "    \n",
    "    # Load the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xls_file_path)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"File converted to CSV and saved at: {csv_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
